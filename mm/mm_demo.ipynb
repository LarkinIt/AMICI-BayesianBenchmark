{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Michaelis-Menten Model Calibration Notebook\n",
    "\n",
    "Based on PTemPest example written in Matlab [here](https://github.com/RuleWorld/ptempest/tree/master/examples/michment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This system describes the 1) reversible binding of an enzyme to substrate and 2) production of substrate product which is defined in the following scheme:  \n",
    "$$E + S \\rightleftharpoons^{k_f}_{k_r} ES \\longrightarrow^{k_{cat}} E + P$$  \n",
    "\n",
    "Assuming total enzyme concentration is significantly smaller than substrate concentration (i.e., $[E]_T \\ll [S]$), the rate is defined as:  \n",
    "$$\\frac{d[P]}{dt} = \\frac{k_{cat}[E]_T[S]}{K_M + [S]}$$\n",
    "where $K_M = \\frac{k_{cat} + k_r}{k_f}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Calibration\n",
    "\n",
    "The following notebook calibrates the Michaelis-Menten model system using synthetically generated data with 1% Gaussian error. Here, we test the following inference methods:\n",
    "1. Metropolis-Hastings (`pyPESTO`)\n",
    "2. Parallel-Tempering MCMC (`pyPESTO`)\n",
    "3. Nested Sampling (`dynesty`)\n",
    "4. Sequential Monte Carlo (`pocoMC`)\n",
    "5. Preconditioned Monte Carlo (`pocoMC`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import roadrunner\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import qmc\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import pocomc as pc\n",
    "import dynesty as dy\n",
    "import pypesto\n",
    "import pypesto.engine as eng\n",
    "import pypesto.sample as sample\n",
    "import pypesto.store as store\n",
    "import pypesto.optimize as optimize\n",
    "from pypesto.ensemble import Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the `Model` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model class has the following attributes:\n",
    "1. `x_n` : `int`\n",
    "    Number of species in the model\n",
    "2. `fit_x0` : `bool`\n",
    "    Whether the initial conditions are to be estimated and are therefore include in `theta` args.\n",
    "3. `x0` : `list[float], optional`\n",
    "    The initial conditions of model species\n",
    "4. `theta_n` : `int`\n",
    "    Number of parameters to be fit. This includes ALL parameters to be estimated which MAY include initial conditions and the standard deviation of the model species. The order of the model parameters in this list is assumed to be as follows:\n",
    "    1. ODE equation parameters\n",
    "    2. Initial conditions (denoted $x_\\#$, **optional**)\n",
    "    3. Species standard deviations (denoted $\\sigma_\\#$)\n",
    "5. `theta_true` : `list[float]` of shape `(theta_n)`\n",
    "    True theta values of the model system. The order of \n",
    "6. `theta_names` : `list[str]` of shape `(theta_n)`\n",
    "    Name of parameters for plotting purposes\n",
    "7. `lower_bnds` : `list[float]` of shape `(theta_n)`\n",
    "    Lower bounds of parameter values \n",
    "8. `upper_bnds` : `list[float]` of shape `(theta_n)`\n",
    "    Upper bounds of parameter values \n",
    "9. `ts` : `list[float]` \n",
    "    Experimental data times \n",
    "10. `data` : `list[float]` of shape `(ts, x_n)` \n",
    "    Experimental data used for model calibration\n",
    "11. `librr_model` : TBD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, opts): #initial settings\n",
    "        for key in opts: #loops for all labels in the list 'key'\n",
    "            setattr(self, key, opts[key]) #creates a dictionary where 'key' are the list of labels & 'ops[key]' are the values\n",
    "\n",
    "    def __call__(self, theta_new):\n",
    "        theta_new = theta_new\n",
    "        res = self.log_likelihood(theta_new)\n",
    "        return res\n",
    "    \n",
    "    def change_and_run(self, model_param, x0):\n",
    "        rr = self.librr_model\n",
    "        rr.resetAll()\n",
    "        rr.integrator.absolute_tolerance = 5e-10\n",
    "        rr.integrator.relative_tolerance = 1e-8\n",
    "    \n",
    "    def call_sim(self, model_param = None, x0=None): #takes in canidate parameters then solves the ode\n",
    "        if model_param is None:\n",
    "            model_param= self.theta_true[:self.ODE_params_n]  #sets the model_params to just the model parameters\n",
    "        if x0 is None:\n",
    "            x0 = self.x0 #if x0 not defined, default x0 to the model x0\n",
    "        t_span = (self.ts[0], self.ts[-1]) #define the time span\n",
    "        x_n = self.x_n \n",
    "        \n",
    "        \n",
    "        \n",
    "        pass\n",
    "        #return result #returns result.t and result.y\n",
    "    \n",
    "    def log_prior(self, theta_new): \n",
    "        bools = [(low <= i <= high) for i,low,high in zip(theta_new, self.lower_bnds, self.upper_bnds)] #if generated values are within bounds\n",
    "        all_in_range = np.all(bools) #if all values are true, then output is true\n",
    "        if all_in_range: #if true\n",
    "            return 0.0 #give 0\n",
    "        return -np.inf #if even one parameter out of bounds, it's false, and returns -infinity\n",
    "\n",
    "    def log_likelihood(self, theta_new): #how good is this canidate parameter fitting my data (maximize it)\n",
    "        model_param = theta_new[:self.ODE_params_n] \n",
    "        if self.fit_x0: \n",
    "            x0 = theta_new[self.ODE_params_n:(self.ODE_params_n + self.x_n)] #sets x0 to 'theta_true' x0 values\n",
    "        else:\n",
    "            x0 = self.x0\n",
    "\n",
    "        if self.fit_sigma:\n",
    "            sigma = theta_new[-len(self.observable_index):] #observable index related to sigma\n",
    "        else:\n",
    "            sigma = [1] * len(self.observable_index) #makes all sigmas default to 1\n",
    "\n",
    "        y = self.run_sim(model_param=model_param, x0=x0) #sets y to the y results of solving ODE\n",
    "        data = self.data #sets data\n",
    "\n",
    "        # Calculate posterior; how good is parameter in terms of fitting the data\n",
    "        term1 = -0.5 * np.log(2*np.pi*np.square(sigma))\n",
    "        term2 = np.square(np.subtract(y, data)) / (2*np.square(sigma))\n",
    "        logLH = np.sum(term1 - term2)\n",
    "        return logLH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the dictionary used to create the Michaelis-Menten problem in the `Model` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Michaelis-Menten Model Options\n",
    "mod_opts = {} #creates a dictionary\n",
    "mod_opts['theta_n'] = 3 #total number of values in big array\n",
    "mod_opts['ODE_params_n'] = 3 # num ODE params\n",
    "mod_opts['x_n'] = 4 # num species\n",
    "mod_opts['sigma_n'] = 0 # num sigmas\n",
    "mod_opts['theta_labels'] = ['$k_1$', '$k_2$', '$k_3$'] # parameter labels\n",
    "\n",
    "mod_opts['fit_x0'] = False # fit initial conditions?\n",
    "mod_opts['x0'] = [10,100,0,0] # initial conditions (if given)\n",
    "mod_opts['observable_index'] = [3] # indices of outputs containing the observables\n",
    "mod_opts['fit_sigma'] = False #fit sigma?\n",
    "mod_opts['theta_true'] = [-4.6052, 0, 0] #guess param values(in log)\n",
    "mod_opts['lower_bnds'] = [-3,-1,-3] #lower bounds(in log)\n",
    "mod_opts['upper_bnds'] = [1,3,3] #upper bounds(in log)\n",
    "\n",
    "# load data for model\n",
    "mod_df = pd.read_csv('michaelis_menten_data.csv', header=0, delimiter=\",\") #reads in data file\n",
    "mod_opts['ts'] = mod_df['t'].values #sets data under 't' in csv as 'ts'\n",
    "data = mod_df['x4'].values\n",
    "mod_opts['data'] = data\n",
    "\n",
    "# Load in SBML model using libroadrunner\n",
    "sbml_file = \"mm_sbml.xml\"\n",
    "librr_model = roadrunner.RoadRunner(sbml_file)\n",
    "librr_theta = [\"log_k1\", \"log_k2\", \"log_k3\"]\n",
    "\n",
    "mod_opts['librr_model'] = librr_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
