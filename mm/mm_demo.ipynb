{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Michaelis-Menten Model Calibration Notebook\n",
    "\n",
    "Based on PTemPest example written in Matlab [here](https://github.com/RuleWorld/ptempest/tree/master/examples/michment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This system describes the 1) reversible binding of an enzyme to substrate and 2) production of substrate product which is defined in the following scheme:  \n",
    "$$E + S \\rightleftharpoons^{k_f}_{k_r} ES \\longrightarrow^{k_{cat}} E + P$$  \n",
    "\n",
    "Assuming total enzyme concentration is significantly smaller than substrate concentration (i.e., $[E]_T \\ll [S]$), the rate is defined as:  \n",
    "$$\\frac{d[P]}{dt} = \\frac{k_{cat}[E]_T[S]}{K_M + [S]}$$\n",
    "where $K_M = \\frac{k_{cat} + k_r}{k_f}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Calibration\n",
    "\n",
    "The following notebook calibrates the Michaelis-Menten model system using synthetically generated data with 1% Gaussian error. Here, we test the following inference methods:\n",
    "1. Metropolis-Hastings (`pyPESTO`)\n",
    "2. Parallel-Tempering MCMC (`pyPESTO`)\n",
    "3. Nested Sampling (`dynesty`)\n",
    "4. Sequential Monte Carlo (`pocoMC`)\n",
    "5. Preconditioned Monte Carlo (`pocoMC`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import roadrunner\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import qmc\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import pypesto\n",
    "import pocomc as pc\n",
    "import dynesty as dy\n",
    "import pypesto.engine as eng\n",
    "import pypesto.sample as sample\n",
    "import pypesto.store as store\n",
    "import pypesto.optimize as optimize\n",
    "from pypesto.ensemble import Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the `Model` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model class has the following attributes:\n",
    "1. `x_n` : `int`\n",
    "    Number of species in the model\n",
    "2. `fit_x0` : `bool`\n",
    "    Whether the initial conditions are to be estimated and are therefore include in `theta` args.\n",
    "3. `x0` : `list[float], optional`\n",
    "    The initial conditions of model species\n",
    "4. `theta_n` : `int`\n",
    "    Number of parameters to be fit. This includes ALL parameters to be estimated which MAY include initial conditions and the standard deviation of the model species. The order of the model parameters in this list is assumed to be as follows:\n",
    "    1. ODE equation parameters\n",
    "    2. Initial conditions (denoted $x_\\#$, **optional**)\n",
    "    3. Species standard deviations (denoted $\\sigma_\\#$)\n",
    "5. `theta_true` : `list[float]` of shape `(theta_n)`\n",
    "    True theta values of the model system. The order of \n",
    "6. `theta_names` : `list[str]` of shape `(theta_n)`\n",
    "    Name of parameters for plotting purposes\n",
    "7. `lower_bnds` : `list[float]` of shape `(theta_n)`\n",
    "    Lower bounds of parameter values \n",
    "8. `upper_bnds` : `list[float]` of shape `(theta_n)`\n",
    "    Upper bounds of parameter values \n",
    "9. `ts` : `list[float]` \n",
    "    Experimental data times \n",
    "10. `data` : `list[float]` of shape `(ts, x_n)` \n",
    "    Experimental data used for model calibration\n",
    "11. `librr_model` : TBD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, opts): #initial settings\n",
    "        for key in opts: #loops for all labels in the list 'key'\n",
    "            setattr(self, key, opts[key]) #creates a dictionary where 'key' are the list of labels & 'ops[key]' are the values\n",
    "\n",
    "    def __call__(self, theta_new):\n",
    "        theta_new = theta_new\n",
    "        res = self.log_likelihood(theta_new)\n",
    "        return res\n",
    "    \n",
    "    def change_and_run(self, model_param, x0):\n",
    "        rr = self.librr_model\n",
    "        rr.resetAll()\n",
    "        rr.integrator.absolute_tolerance = 5e-10\n",
    "        rr.integrator.relative_tolerance = 1e-8\n",
    "\n",
    "        for spec_name, val in zip(self.librr_species, x0):\n",
    "            init_species_string = f\"init([{spec_name}])\"\n",
    "            rr[init_species_string] = val\n",
    "            rr.reset()\n",
    "        \n",
    "        for name, value in zip(self.librr_theta, model_param):\n",
    "            rr[name] = value\n",
    "            rr.reset() # forces initial conditions and BNGL functions to be re-evaluated\n",
    "        \n",
    "        t_span = (self.ts[0], self.ts[-1])\n",
    "        trajs = rr.simulate(t_span[0], t_span[1], int(t_span[1]*100+1))\n",
    "        return trajs    \n",
    "        \n",
    "    def call_sim(self, model_param = None, x0 = None, return_all_species=False): #takes in candidate parameters then solves the ode\n",
    "        if model_param is None:\n",
    "            model_param= self.theta_true[:self.ODE_params_n]  \n",
    "        if x0 is None:\n",
    "            x0 = self.x0 \n",
    "        \n",
    "        trajs = self.change_and_run(model_param, x0)\n",
    "        if return_all_species:\n",
    "            return trajs\n",
    "        \n",
    "        sim_ts = trajs[:, 0]\n",
    "        species = trajs[:, 1:]\n",
    "        # ! TO DO: Assumes simulation includes data ts which is not always the case\n",
    "        # ! Need to add interpolation for times not included in simulation\n",
    "        t_idxs = np.where(np.in1d(sim_ts, self.ts))[0]\n",
    "        return_species = species[t_idxs, self.observable_idxs]\n",
    "        return return_species\n",
    "    \n",
    "    def log_prior(self, theta_new): \n",
    "        bools = [(low <= i <= high) for i,low,high in zip(theta_new, self.lower_bnds, self.upper_bnds)] #if generated values are within bounds\n",
    "        all_in_range = np.all(bools) #if all values are true, then output is true\n",
    "        if all_in_range: \n",
    "            return 0.0 \n",
    "        return -np.inf #if even one parameter out of bounds, it's false, and returns -infinity\n",
    "\n",
    "    def log_likelihood(self, theta_new): #how good is this canidate parameter fitting my data (maximize it)\n",
    "        model_param = theta_new[:self.ODE_params_n] \n",
    "        if self.fit_x0: \n",
    "            x0 = theta_new[self.ODE_params_n:(self.ODE_params_n + self.x_n)] #sets x0 to 'theta_true' x0 values\n",
    "        else:\n",
    "            x0 = self.x0\n",
    "\n",
    "        if self.fit_sigma:\n",
    "            sigma = theta_new[-len(self.observable_idxs):] #observable index related to sigma\n",
    "        else:\n",
    "            sigma = [1] * len(self.observable_idxs) #makes all sigmas default to 1\n",
    "        y = self.call_sim(model_param=model_param, x0=x0) #sets y to the y results of solving ODE\n",
    "        data = self.data #sets data\n",
    "\n",
    "        # Calculate likelihood\n",
    "        term1 = -0.5 * np.log(2*np.pi*np.square(sigma))\n",
    "        term2 = np.square(np.subtract(y, data)) / (2*np.square(sigma))\n",
    "        logLH = np.sum(term1 - term2)\n",
    "        return logLH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the dictionary used to create the Michaelis-Menten problem in the `Model` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Michaelis-Menten Model Options\n",
    "mod_opts = {} #creates a dictionary\n",
    "mod_opts['theta_n'] = 3 #total number of values in big array\n",
    "mod_opts['ODE_params_n'] = 3 # num ODE params\n",
    "mod_opts['x_n'] = 4 # num species\n",
    "mod_opts['sigma_n'] = 0 # num sigmas\n",
    "mod_opts['theta_labels'] = ['$k_1$', '$k_2$', '$k_3$'] # parameter labels\n",
    "\n",
    "# ! TO DO: we currently assume it fit_x0, then ALL species initial conditions are fit\n",
    "mod_opts['fit_x0'] = False # fit initial conditions?\n",
    "mod_opts['x0'] = [600,6,0,0] # initial conditions (if given)\n",
    "mod_opts['observable_idxs'] = [3] # indices of outputs containing the observables\n",
    "mod_opts['fit_sigma'] = False #fit sigma?\n",
    "mod_opts['theta_true'] = [-2.77, -1.0, -2.0] #guess param values(in log)\n",
    "mod_opts['lower_bnds'] = [-3,-1,-3] #lower bounds(in log)\n",
    "mod_opts['upper_bnds'] = [1,3,3] #upper bounds(in log)\n",
    "\n",
    "# load data for model\n",
    "mod_df = pd.read_csv('mm_data.csv', header=0, delimiter=\",\") #reads in data file\n",
    "mod_opts['ts'] = mod_df['t'].values #sets data under 't' in csv as 'ts'\n",
    "data = mod_df['P'].values\n",
    "mod_opts['data'] = data\n",
    "# Load in SBML model using libroadrunner\n",
    "sbml_file = \"mm_sbml.xml\"\n",
    "librr_model = roadrunner.RoadRunner(sbml_file)\n",
    "librr_theta = [\"log_k1\", \"log_k2\", \"log_k3\"]\n",
    "librr_species = [\"S1\", \"S2\", \"S3\", \"S4\"]\n",
    "librr_labels = [\"Substrate\", \"Enzyme\", \"Substrate-Enzyme\", \"Product\"]\n",
    "\n",
    "mod_opts['librr_model'] = librr_model\n",
    "mod_opts['librr_theta'] = librr_theta\n",
    "mod_opts[\"librr_species\"] = librr_species\n",
    "\n",
    "mod = Model(mod_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_trajs = mod.call_sim(return_all_species=True)\n",
    "bad_trajs = mod.call_sim(x0=[200,6,0,300], model_param=[0.0,2.0,-2.0], return_all_species=True)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(true_trajs[:, 0], true_trajs[:, 4], label=\"True Solution\")\n",
    "plt.plot(bad_trajs[:, 0], bad_trajs[:, 4], label=\"Poor Fit\")\n",
    "plt.plot(mod.ts, mod.data, 'go', label=\"Synthetic Data\")\n",
    "plt.xlabel(\"Time\");\n",
    "plt.ylabel(\"[P]\");\n",
    "plt.legend(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks with log_prior\n",
    "test_theta = [-3,3,3] # This should return 0.0 \n",
    "print(mod.log_prior(test_theta))\n",
    "test_bad_theta1 = [-4, 1, 1] # should return -np.inf\n",
    "print(mod.log_prior(test_bad_theta1))\n",
    "test_bad_theta2 = [1, 4, 3] # should return -np.inf\n",
    "print(mod.log_prior(test_bad_theta2))\n",
    "test_bad_theta3 = [0, 0, 3.1] # should return -np.inf\n",
    "print(mod.log_prior(test_bad_theta3))\n",
    "print(\"--------------------\")\n",
    "# Sanity checks with log_likelihood\n",
    "print(mod.log_likelihood(mod.theta_true))\n",
    "print(mod.log_likelihood([-20,2,-5])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
