{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Michaelis-Menten Model Calibration Notebook\n",
    "\n",
    "Based on PTemPest example written in Matlab [here](https://github.com/RuleWorld/ptempest/tree/master/examples/michment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This system describes the 1) reversible binding of an enzyme to substrate and 2) production of substrate product which is defined in the following scheme:  \n",
    "$$E + S \\rightleftharpoons^{k_f}_{k_r} ES \\longrightarrow^{k_{cat}} E + P$$  \n",
    "\n",
    "Assuming total enzyme concentration is significantly smaller than substrate concentration (i.e., $[E]_T \\ll [S]$), the rate is defined as:  \n",
    "$$\\frac{d[P]}{dt} = \\frac{k_{cat}[E]_T[S]}{K_M + [S]}$$\n",
    "where $K_M = \\frac{k_{cat} + k_r}{k_f}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Calibration\n",
    "\n",
    "The following notebook calibrates the Michaelis-Menten model system using synthetically generated data with 1% Gaussian error. \n",
    "\n",
    "Here, we test the following inference methods:\n",
    "1. Metropolis-Hastings (`pyPESTO`)\n",
    "2. Parallel-Tempering MCMC (`pyPESTO`)\n",
    "3. Nested Sampling (`dynesty`)\n",
    "4. Sequential Monte Carlo (`pocoMC`)\n",
    "5. Preconditioned Monte Carlo (`pocoMC`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This machine has 8 CPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import roadrunner\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import qmc\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import pypesto\n",
    "import pocomc as pc\n",
    "import dynesty as dy\n",
    "import pypesto.engine as eng\n",
    "import pypesto.sample as sample\n",
    "import pypesto.store as store\n",
    "import pypesto.optimize as optimize\n",
    "from pypesto.ensemble import Ensemble\n",
    "\n",
    "SEED = 1\n",
    "np.random.seed(seed=SEED)\n",
    "\n",
    "n_cpus = os.cpu_count() \n",
    "print('This machine has {} CPUs'.format(n_cpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the `Model` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model class has the following attributes:\n",
    "1. `x_n` : `int`<br>\n",
    "    Number of species in the model\n",
    "2. `fit_x0` : `bool`<br>\n",
    "    Whether the initial conditions are to be estimated and are therefore included in `theta` args.\n",
    "3. `fit_sigma` : `bool`<br>\n",
    "   Whether to estimate the standard deviation of the experimental data which is also used in the `log_likelihood()` function\n",
    "4. `x0` : `list[float], optional`<br>\n",
    "   The initial conditions of model species\n",
    "5. `ODE_params_n` : `int`<br>\n",
    "   Number of parameters used in ODEs/SBML model\n",
    "6. `theta_n` : `int`<br>\n",
    "    Number of parameters to be fit. This includes ALL parameters to be estimated which MAY include initial conditions and the standard deviation of the model species.<br> The order of the model parameters in this list is assumed to be as follows:<br>\n",
    "    1. ODE equation parameters\n",
    "    2. Initial conditions (denoted $x_\\#$, **optional**)\n",
    "    3. Species standard deviations (denoted $\\sigma_\\#$)\n",
    "7. `theta_true` : `list[float]` of shape `(theta_n)`<br>\n",
    "    True theta values of the model system. \n",
    "8. `lower_bnds` : `list[float]` of shape `(theta_n)`<br>\n",
    "    Lower bounds of parameter values \n",
    "9.  `upper_bnds` : `list[float]` of shape `(theta_n)`<br>\n",
    "    Upper bounds of parameter values \n",
    "10. `ts` : `list[float]`<br> \n",
    "    Experimental data times \n",
    "11. `data` : `list[float]` of shape `(len(ts), x_n)`<br> \n",
    "    Experimental data used for model calibration\n",
    "12. `librr_model` : `<class 'roadrunner.roadrunner.RoadRunner'>`<br>\n",
    "    `libroadrunner` model object loaded from SBML file of the model\n",
    "13. `librr_theta` : `list[str]`<br>\n",
    "    The names of the ODE parameters as specified in the SBML file. These are used to actually change the parameters during calibration so they _must_ match the parameter names in the SBML file.\n",
    "14. `librr_species` : `list[str]`<br>\n",
    "    Names of the species ID defined in the SBML file. Note: you must use the SBML file `species` ID and not the name (eg. `S1` for the substrate and `S2` for the enzyme). The length of this list must be equal to `x_n`\n",
    "15. `librr_labels` : `list[str]`<br>\n",
    "    Labels used for plotting purposes of all the model species. This _could_ be the same as the `species` name from the SBML file but that is up to the user. The length of this list must be equal to `x_n`\n",
    "16. `observable_idxs` : `list[int]`<br> \n",
    "    The indices of `librr_species` that are used to compare fit to experimental data. The length of `observable_idxs` _must_ be $\\leq$ `x_n`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Model` class assumptions\n",
    "\n",
    "The species order of the following is assumed to be the same:\n",
    "1. `librr_labels`\n",
    "2. `librr_species`\n",
    "3. `x0`\n",
    "4. `data`\n",
    "\n",
    "The parameter order of the following is assumed to be the same:\n",
    "1. `theta_true`\n",
    "2. `lower_bnds`\n",
    "3. `upper_bnds`\n",
    "4. `librr_theta` (only includes ODE parameters)\n",
    "\n",
    "The prior is assumed to be log-uniform given lower and upper bounds. \n",
    "\n",
    "The log likelihood function assumes the data is normally distributed and is thus is calculated as:\n",
    "\n",
    "$$P($$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, opts): #initial settings\n",
    "        for key in opts: #loops for all labels in the list 'key'\n",
    "            setattr(self, key, opts[key]) #creates a dictionary where 'key' are the list of labels & 'ops[key]' are the values\n",
    "\n",
    "    def __call__(self, theta_new):\n",
    "        theta_new = theta_new\n",
    "        res = self.log_likelihood(theta_new)\n",
    "        return res\n",
    "    \n",
    "    def change_and_run(self, model_param, x0):\n",
    "        rr = self.librr_model\n",
    "        rr.resetAll()\n",
    "        rr.integrator.absolute_tolerance = 5e-10\n",
    "        rr.integrator.relative_tolerance = 1e-8\n",
    "\n",
    "        for spec_name, val in zip(self.librr_species, x0):\n",
    "            init_species_string = f\"init([{spec_name}])\"\n",
    "            rr[init_species_string] = float(val)\n",
    "            rr.reset()\n",
    "        \n",
    "        for name, value in zip(self.librr_theta, model_param):\n",
    "            rr[name] = float(value)\n",
    "            rr.reset() # forces initial conditions and BNGL functions to be re-evaluated\n",
    "        \n",
    "        t_span = (self.ts[0], self.ts[-1])\n",
    "        trajs = rr.simulate(t_span[0], t_span[1], int(t_span[1]*100+1))\n",
    "        return trajs    \n",
    "        \n",
    "    def call_sim(self, model_param = None, x0 = None, return_all_species=False): #takes in candidate parameters then solves the ode\n",
    "        if model_param is None:\n",
    "            model_param= self.theta_true[:self.ODE_params_n]  \n",
    "        if x0 is None:\n",
    "            x0 = self.x0 \n",
    "        \n",
    "        trajs = self.change_and_run(model_param, x0)\n",
    "        if return_all_species:\n",
    "            return trajs\n",
    "        \n",
    "        sim_ts = trajs[:, 0]\n",
    "        species = trajs[:, 1:]\n",
    "        # ! TO DO: Assumes simulation includes data ts which is not always the case\n",
    "        # ! Need to add interpolation for times not included in simulation\n",
    "        t_idxs = np.where(np.in1d(sim_ts, self.ts))[0]\n",
    "        return_species = species[t_idxs, self.observable_idxs]\n",
    "        return return_species\n",
    "    \n",
    "    def log_prior(self, theta_new): \n",
    "        bools = [(low <= i <= high) for i,low,high in zip(theta_new, self.lower_bnds, self.upper_bnds)] #if generated values are within bounds\n",
    "        all_in_range = np.all(bools) #if all values are true, then output is true\n",
    "        if all_in_range: \n",
    "            return 0.0 \n",
    "        return -np.inf #if even one parameter out of bounds, it's false, and returns -infinity\n",
    "\n",
    "    def log_likelihood(self, theta_new): #how good is this candidate parameter fitting my data (maximize it)\n",
    "        model_param = theta_new[:self.ODE_params_n] \n",
    "        if self.fit_x0: \n",
    "            x0 = theta_new[self.ODE_params_n:(self.ODE_params_n + self.x_n)] #sets x0 to 'theta_true' x0 values\n",
    "        else:\n",
    "            x0 = self.x0\n",
    "\n",
    "        if self.fit_sigma:\n",
    "            sigma = theta_new[-len(self.observable_idxs):] #observable index related to sigma\n",
    "        else:\n",
    "            sigma = [1] * len(self.observable_idxs) #makes all sigmas default to 1\n",
    "        y = self.call_sim(model_param=model_param, x0=x0) #sets y to the y results of solving ODE\n",
    "        data = self.data #sets data\n",
    "\n",
    "        # Calculate likelihood\n",
    "        term1 = -0.5 * np.log(2*np.pi*np.square(sigma))\n",
    "        term2 = np.square(np.subtract(y, data)) / (2*np.square(sigma))\n",
    "        logLH = np.sum(term1 - term2)\n",
    "        return np.array(logLH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the dictionary used to create the Michaelis-Menten problem in the `Model` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'roadrunner.roadrunner.RoadRunner'>\n"
     ]
    }
   ],
   "source": [
    "# Michaelis-Menten Model Options\n",
    "mod_opts = {} #creates a dictionary\n",
    "mod_opts['theta_n'] = 3 #total number of values in big array\n",
    "mod_opts['ODE_params_n'] = 3 # num ODE params\n",
    "mod_opts['x_n'] = 4 # num species\n",
    "\n",
    "# ! TO DO: we currently assume it fit_x0, then ALL species initial conditions are fit\n",
    "mod_opts['fit_x0'] = False # fit initial conditions?\n",
    "mod_opts['x0'] = [600,6,0,0] # initial conditions (if given)\n",
    "mod_opts['observable_idxs'] = [3] # indices of outputs containing the observables\n",
    "mod_opts['fit_sigma'] = False #fit sigma?\n",
    "mod_opts['theta_true'] = [-2.77, -1.0, -2.0] #guess param values(in log)\n",
    "mod_opts['lower_bnds'] = [-3.0,-1.0,-3.0] #lower bounds(in log)\n",
    "mod_opts['upper_bnds'] = [1.0,3.0,3.0] #upper bounds(in log)\n",
    "\n",
    "# load data for model\n",
    "mod_df = pd.read_csv('mm_data.csv', header=0, delimiter=\",\") #reads in data file\n",
    "mod_opts['ts'] = mod_df['t'].values #sets data under 't' in csv as 'ts'\n",
    "data = mod_df['P'].values\n",
    "mod_opts['data'] = data\n",
    "# Load in SBML model using libroadrunner\n",
    "sbml_file = \"mm_sbml.xml\"\n",
    "librr_model = roadrunner.RoadRunner(sbml_file)\n",
    "librr_theta = [\"log_k1\", \"log_k2\", \"log_k3\"]\n",
    "librr_species = [\"S1\", \"S2\", \"S3\", \"S4\"]\n",
    "librr_labels = [\"Substrate\", \"Enzyme\", \"Substrate-Enzyme\", \"Product\"]\n",
    "print(type(librr_model))\n",
    "mod_opts['librr_model'] = librr_model\n",
    "mod_opts['librr_theta'] = librr_theta\n",
    "mod_opts[\"librr_species\"] = librr_species\n",
    "\n",
    "mod = Model(mod_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_trajs = mod.call_sim(return_all_species=True)\n",
    "bad_trajs = mod.call_sim(x0=[200,6,0,300], model_param=[0.0,2.0,-2.0], return_all_species=True)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(true_trajs[:, 0], true_trajs[:, 4], label=\"True Solution\")\n",
    "plt.plot(bad_trajs[:, 0], bad_trajs[:, 4], label=\"Poor Fit\")\n",
    "plt.plot(mod.ts, mod.data, 'go', label=\"Synthetic Data\")\n",
    "plt.xlabel(\"Time\");\n",
    "plt.ylabel(\"[P]\");\n",
    "plt.legend(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks with log_prior\n",
    "test_theta = [-3,3,3] # This should return 0.0 \n",
    "print(mod.log_prior(test_theta))\n",
    "test_bad_theta1 = [-4, 1, 1] # should return -np.inf\n",
    "print(mod.log_prior(test_bad_theta1))\n",
    "test_bad_theta2 = [1, 4, 3] # should return -np.inf\n",
    "print(mod.log_prior(test_bad_theta2))\n",
    "test_bad_theta3 = [0, 0, 3.1] # should return -np.inf\n",
    "print(mod.log_prior(test_bad_theta3))\n",
    "print(\"--------------------\")\n",
    "# Sanity checks with log_likelihood\n",
    "print(mod.log_likelihood(mod.theta_true))\n",
    "print(mod.log_likelihood([-20,2,-5])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Calibration\n",
    "\n",
    "### Initialize samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_particles = 1000\n",
    "\n",
    "# Initialise particles' positions using samples from the prior \n",
    "sampler = qmc.LatinHypercube(d=mod.theta_n, seed=SEED)\n",
    "sample = sampler.random(n=n_particles) \n",
    "print(\"The discrepancy of the sampling (i.e., sample quality): %.4f\"%qmc.discrepancy(sample)) #discrepancy is distance between cont. uniform distr. on hypercube & discr. uniform distr. on n distinct sample points\n",
    "prior_samples = qmc.scale(sample, l_bounds=mod.lower_bnds, u_bounds=mod.upper_bnds)\n",
    "print(prior_samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run `pocoMC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time() #stores current time, date, year, etc. in one float\n",
    "with Pool(n_cpus) as pool: #sets up code to run over my n number of cpus on laptop\n",
    "    \n",
    "    sampler = pc.Sampler(n_particles = n_particles,\n",
    "                    n_dim = mod.theta_n,\n",
    "                    log_likelihood = mod.log_likelihood,\n",
    "                    log_prior = mod.log_prior,\n",
    "                    bounds = np.array(list(zip(mod.lower_bnds, mod.upper_bnds))),\n",
    "                    pool = pool,\n",
    "                    random_state=SEED,\n",
    "                    vectorize_likelihood=False,\n",
    "                    vectorize_prior=False,\n",
    "                    infer_vectorization=False\n",
    "                    ) #stores all relevant info from # of parameters being fit (ndim) to the actual results\n",
    "\n",
    "    sampler.run(prior_samples = prior_samples) #starts with prior sample definied in latin hypercube sampling, and runs it\n",
    "    result = sampler.results #results of sampler.run on prior_samples\n",
    "\n",
    "\n",
    "with open('tester_result.pkl', 'wb') as f: #open that file if exists, if not make that file\n",
    "    pickle.dump(result, f, protocol=pickle.HIGHEST_PROTOCOL) #saves result object (dictionary) to pickle file\n",
    "\n",
    "with open('tester_mod.pkl', 'wb') as f: #open that file if exists, if not make that file\n",
    "    pickle.dump(mod.__dict__, f, protocol=pickle.HIGHEST_PROTOCOL) #saves model dictionary in pickle file\n",
    "\n",
    "t1 = time.time() #time after running this section\n",
    "seconds = t1-t0 #difference in start and stop time\n",
    "\n",
    "elapsed = time.strftime(\"%H:%M:%S\", time.gmtime(seconds)) #converts float to a time quantity we use\n",
    "print('\\nElapsed time: ', elapsed) #printing time it took for code to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
